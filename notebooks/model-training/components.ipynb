{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "import torch.utils.data as data\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vad.architectures import STAD\n",
    "from vad.datasets import TrajectoryDataset, ExactBatchSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T07:18:29.972058Z",
     "start_time": "2025-04-17T07:18:29.967461Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LCIVs9481Ais",
    "outputId": "b726bc73-9411-4201-8d74-dc59ac14551b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "torch.set_num_threads(8)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of Gaussian Mixture Model Components:\t[1, 5, 10, 20, 25, 30, 40, 50, 100]\n",
      "Hidden Dimensions of GMM:\t\t\t[1, 8, 16, 32, 40, 48, 64, 80, 160]\n"
     ]
    }
   ],
   "source": [
    "# Experiment parameters\n",
    "exp_type = 'components'\n",
    "include_weather = True\n",
    "n_weather_vars = 5\n",
    "n_components_gmm = [1, 5, 10, 20, 25, 30, 40, 50, 100]\n",
    "hidden_dims_gmm = [int(((n_components_gmm[i] - 20) / 20) * 32 + 32) for i in range(len(n_components_gmm))]\n",
    "\n",
    "print(\n",
    "    f'Numbers of Gaussian Mixture Model Components:\\t{n_components_gmm}',\n",
    "    f'Hidden Dimensions of GMM:\\t\\t\\t{hidden_dims_gmm}', sep='\\n'\n",
    ")\n",
    "\n",
    "# Training parameters\n",
    "epochs = 100\n",
    "patience = epochs\n",
    "learning_rate = 1e-5 # Peak LR\n",
    "embed_dim = 32\n",
    "latent_dim_ae = 32\n",
    "weight_decay = 0.1\n",
    "dropout = 0.1\n",
    "n_head_te = 8\n",
    "n_layers_te = 4\n",
    "eps_gmm = 1e-7\n",
    "eps_loss = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCHleP5KDn-F"
   },
   "source": [
    "# STAD Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gmm_penalty(sigma, epsilon=eps_loss):\n",
    "    \"\"\"\n",
    "    Vectorized computation of GMM penalty (sum of reciprocals of diagonal elements)\n",
    "\n",
    "    sigma: Component covariances. Shape: [num_components, input_dim, input_dim]\n",
    "    epsilon: Small value for numerical stability\n",
    "    \"\"\"\n",
    "    # Extract diagonal elements from all covariance matrices at once\n",
    "    # Shape: [num_components, input_dim]\n",
    "    diag_elements = torch.diagonal(sigma, dim1=-2, dim2=-1)\n",
    "\n",
    "    # Add epsilon for numerical stability before taking reciprocal\n",
    "    # This prevents division by very small numbers\n",
    "    penalty = torch.sum(1.0 / (diag_elements + epsilon))\n",
    "\n",
    "    return penalty\n",
    "\n",
    "def compute_full_loss(penalty, transformer_loss, energy, d,\n",
    "                      lambda_1=1, lambda_2=1, lambda_3=5e-3):\n",
    "                      # λ₁=1, λ₂=1, λ₃=0.005 as in the STAD publication\n",
    "    return transformer_loss + (lambda_1 * energy) + (lambda_2 * d) + (lambda_3 * penalty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STAD Unbiased Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_training_set(model, train_dataloader, device):\n",
    "    \"\"\"\n",
    "    Evaluate the training set with model in eval mode to get unbiased loss.\n",
    "    Returns the average training loss without gradients or dropout effects.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(train_dataloader, desc=\"Evaluating Training Set\"):\n",
    "            # Move data to device\n",
    "            inputs = {k: v.to(device) for k, v in batch.get('src_window').items()}\n",
    "            targets = {k: v.to(device) for k, v in batch.get('tgt_window').items()}\n",
    "            weather_stats = batch.get('weather_stats', None).to(device)\n",
    "\n",
    "            # Forward pass (will use testing=False path due to eval mode)\n",
    "            l, energy, d_h, sigma = model(inputs, targets, weather_stats)\n",
    "            l, energy, d_h = l.mean(), energy.mean(), d_h.mean()\n",
    "\n",
    "            # Calculate loss components\n",
    "            penalty = calculate_gmm_penalty(sigma)\n",
    "            penalty = penalty.mean()\n",
    "\n",
    "            # Compute final loss\n",
    "            train_eval_loss = compute_full_loss(penalty, l, energy, d_h).mean()\n",
    "\n",
    "            # Accumulate loss\n",
    "            total_train_loss += train_eval_loss.item()\n",
    "\n",
    "    # Return average loss\n",
    "    return total_train_loss / len(train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STAD Validation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, device):\n",
    "\n",
    "    total_val_loss = 0\n",
    "    total_energy = 0\n",
    "    total_te_loss = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for batchidx, batch in enumerate(tqdm(dataloader, desc=\"Validation\")):\n",
    "\n",
    "        # Move data to device\n",
    "        inputs = {k: v.to(device) for k, v in batch.get('src_window').items()}\n",
    "        targets = {k: v.to(device) for k, v in batch.get('tgt_window').items()}\n",
    "        weather_stats = batch.get('weather_stats', None).to(device)\n",
    "\n",
    "        # Pass data to model\n",
    "        l, energy, d_h, sigma = model(inputs, targets, weather_stats)\n",
    "        l, energy, d_h = l.mean(), energy.mean(), d_h.mean()\n",
    "\n",
    "        # Calculate loss components\n",
    "        penalty = calculate_gmm_penalty(sigma)\n",
    "        penalty = penalty.mean()\n",
    "\n",
    "        # Compute the final loss\n",
    "        stad_loss = compute_full_loss(penalty, l, energy, d_h)\n",
    "        stad_loss = stad_loss.mean()\n",
    "\n",
    "        # Update total validation loss and total energy\n",
    "        total_val_loss += stad_loss.item()\n",
    "        total_energy += energy.item()\n",
    "        total_te_loss += l.item()\n",
    "\n",
    "    # Calculate average validation loss and energy\n",
    "    avg_val_loss = total_val_loss / len(dataloader)\n",
    "    avg_energy = total_energy / len(dataloader)\n",
    "    avg_te_loss = total_te_loss / len(dataloader)\n",
    "    return avg_val_loss, avg_energy, avg_te_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KTx9iY5XDn-F"
   },
   "source": [
    "# STAD Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,\n",
    "          train_dataloader,\n",
    "          valid_dataloader,\n",
    "          optimizer,\n",
    "          scheduler,\n",
    "          num_epochs,\n",
    "          device,\n",
    "          patience,\n",
    "          save_dir='./models'):\n",
    "\n",
    "    # Create directory\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Initialize TensorBoard writer\n",
    "    timestamp = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "    writer = SummaryWriter(log_dir=f'./runs/{timestamp}_{experiment_name}')\n",
    "\n",
    "    # Initialize variables for early stopping\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        # Training phase\n",
    "        for batchidx, batch in enumerate(tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")):\n",
    "\n",
    "            # Move data to device\n",
    "            inputs = {k: v.to(device) for k, v in batch.get('src_window').items()}\n",
    "            targets = {k: v.to(device) for k, v in batch.get('tgt_window').items()}\n",
    "            weather_stats = batch.get('weather_stats', None).to(device)\n",
    "\n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Pass data to model\n",
    "            l, energy, d_h, sigma = model(inputs, targets, weather_stats)\n",
    "            l, energy, d_h = l.mean(), energy.mean(), d_h.mean()\n",
    "\n",
    "            # Calculate loss components\n",
    "            penalty = calculate_gmm_penalty(sigma)\n",
    "            penalty = penalty.mean()\n",
    "\n",
    "            # Compute the final loss\n",
    "            stad_loss = compute_full_loss(penalty, l, energy, d_h).mean()\n",
    "\n",
    "            # Update total loss for epoch\n",
    "            train_loss += stad_loss.mean()\n",
    "\n",
    "            # Print progress\n",
    "            if batchidx % 200 == 0:\n",
    "                writer.add_scalar('Batch/te_loss', l, epoch * len(train_dataloader) + batchidx)\n",
    "                writer.add_scalar('Batch/Energy', energy, epoch * len(train_dataloader) + batchidx)\n",
    "                writer.add_scalar('Batch/train_loss', stad_loss, epoch * len(train_dataloader) + batchidx)\n",
    "                writer.add_scalar('Batch/Penalty', penalty*0.005, epoch * len(train_dataloader) + batchidx)\n",
    "                print(f'Batch {batchidx}/{len(train_dataloader)} | Loss: {stad_loss:.6f}')\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            stad_loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            train_loss = train_loss.detach()\n",
    "\n",
    "        # Calculate average training loss for this epoch\n",
    "        avg_train_loss = train_loss / len(train_dataloader)\n",
    "        true_loss = evaluate_training_set(model, train_dataloader, device) # already averaged\n",
    "\n",
    "        # Validation phase\n",
    "        val_loss, avg_energy, avg_te_loss = validate(model, valid_dataloader, device)\n",
    "\n",
    "        # Log metrics to TensorBoard\n",
    "        writer.add_scalar('Epoch/train_loss', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Epoch/validation_loss', val_loss, epoch)\n",
    "        writer.add_scalar('Epoch/avg_energy', avg_energy, epoch)\n",
    "        writer.add_scalar('Epoch/avg_te_loss', avg_te_loss, epoch)\n",
    "        writer.add_scalar('Epoch/learning_rate', scheduler.get_last_lr()[0], epoch)\n",
    "        writer.add_scalar('Epoch/true_loss', true_loss, epoch)\n",
    "\n",
    "        # Print epoch summary\n",
    "        print(f'Epoch {epoch+1}/{num_epochs} | Average Train Loss: {avg_train_loss:.6f} | Average Validation Loss: {val_loss:.6f}')\n",
    "\n",
    "        # Save latest model\n",
    "        latest_model_path = os.path.join(save_dir, 'STAD_latest.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': avg_train_loss,\n",
    "            'val_loss': val_loss\n",
    "        }, latest_model_path)\n",
    "\n",
    "        # Check if this is the best model so far\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "\n",
    "            # Save best model\n",
    "            best_model_path = os.path.join(save_dir, 'STAD_best.pth')\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': val_loss\n",
    "            }, best_model_path)\n",
    "            print(f\"Saved new best model with validation loss: {val_loss:.6f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"Validation loss did not improve. Patience: {patience_counter}/{patience}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs!\")\n",
    "            break\n",
    "\n",
    "        # Free cached memory\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Close TensorBoard writer\n",
    "    writer.close()\n",
    "\n",
    "    # Load the best model\n",
    "    checkpoint = torch.load(best_model_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Loaded best model from epoch {checkpoint['epoch']+1} with validation loss: {checkpoint['val_loss']:.6f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_dataset_train = TrajectoryDataset(ds_type='train',\n",
    "                                       lat_bins=400,\n",
    "                                       lon_bins=400,\n",
    "                                       sog_bins=30,\n",
    "                                       cog_bins=72,\n",
    "                                       file_directory='../../data',\n",
    "                                       filename='joined-train-stad-weather.pkl',\n",
    "                                       include_weather=include_weather)\n",
    "\n",
    "traj_dataset_valid = TrajectoryDataset(ds_type='valid',\n",
    "                                       lat_bins=400,\n",
    "                                       lon_bins=400,\n",
    "                                       sog_bins=30,\n",
    "                                       cog_bins=72,\n",
    "                                       file_directory='../../data',\n",
    "                                       filename='joined-valid-stad-weather.pkl',\n",
    "                                       include_weather=include_weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_sampler = ExactBatchSampler(traj_dataset_train.batch_boundaries, shuffle_batches=True)\n",
    "valid_batch_sampler = ExactBatchSampler(traj_dataset_valid.batch_boundaries, shuffle_batches=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_train = data.DataLoader(traj_dataset_train, batch_sampler=train_batch_sampler, num_workers=4, pin_memory=True, persistent_workers=True)\n",
    "data_loader_valid = data.DataLoader(traj_dataset_valid, batch_sampler=valid_batch_sampler, num_workers=4, pin_memory=True, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For each num of gaussian components:\n",
    "for n_components, hidden_dim_gmm in tqdm(zip(n_components_gmm, hidden_dims_gmm), desc='Running Experiment'):\n",
    "\n",
    "    experiment_name = f'{exp_type}_epochs_{epochs}_embed_{embed_dim}_wd_{weight_decay}_lr_{learning_rate}_hgmm_{hidden_dim_gmm}_lae_{latent_dim_ae}_comp_{n_components}_{datetime.now().strftime('%b%d_%H-%M-%S')}'\n",
    "    print(experiment_name)\n",
    "\n",
    "    # - Create a model\n",
    "    stad = STAD(\n",
    "        n_lat_bins=400,\n",
    "        n_lon_bins=400,\n",
    "        n_sog_bins=30,\n",
    "        n_cog_bins=72,\n",
    "        max_seq_len=10,\n",
    "        embed_dim=embed_dim,\n",
    "        dropout=dropout,\n",
    "        nhead_te=n_head_te,\n",
    "        n_layers_te=n_layers_te,\n",
    "        latent_dim_ae=latent_dim_ae,\n",
    "        n_weather_vars=n_weather_vars,\n",
    "        hidden_dim_gmm=hidden_dim_gmm,\n",
    "        eps_gmm=eps_gmm,\n",
    "        n_components_gmm=n_components).to(device)\n",
    "\n",
    "    print(stad)\n",
    "\n",
    "    # Setup optimizer\n",
    "    optimizer = AdamW(stad.parameters(),\n",
    "    #                 betas=(0.5, 0.999), # Lower b1 because of variation in batch (trajectory) length\n",
    "                     weight_decay=weight_decay)\n",
    "\n",
    "    # Setup LR Scheduler\n",
    "    scheduler = OneCycleLR(optimizer,\n",
    "                        max_lr=learning_rate,            # Peak learning rate\n",
    "                        epochs=epochs,\n",
    "                        steps_per_epoch=len(data_loader_train),\n",
    "                        anneal_strategy='cos'\n",
    "    )\n",
    "\n",
    "    # Train the model for same number of epochs and save the model\n",
    "    final_model = train(stad,\n",
    "                        data_loader_train,\n",
    "                        data_loader_valid,\n",
    "                        optimizer=optimizer,\n",
    "                        scheduler=scheduler,\n",
    "                        num_epochs=epochs,\n",
    "                        device=device,\n",
    "                        patience=patience,\n",
    "                        save_dir=f'./models/{experiment_name}')\n",
    "\n",
    "    del stad, final_model, optimizer, scheduler"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
